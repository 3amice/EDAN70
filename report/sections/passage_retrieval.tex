The purpose of the passage retriever is to reduce the information to a size that is more manageable.
Given a question, the passage retriever translates this into a query, and searches the index for passages 
relevant to this query. Where these passages are sorted by their similarity to the query.
The passage retriver is entirely built upon Lucene Core, an open source, Java-based system, 
that is capable of fast and effective indexing, and smart querying.

\subsection{Wikipedia}
As mentioned, the Swedish Wikipedia is used as information source, downloaded from Wikimedia {reference here} as wikitext embedded in XML.
To gather the useful text, a python script were used to simply remove everything but the text. 
This script were slightly modified to accomodate both indexing by entire articles, and indexing by paragraphs (article subsections).

\subsection{Lucene}

\subsubsection{Analyzer}
Lucene comes with many analyzers, adapted to different languages. 
These analyzers help lucene to parse text and provide stemming of words.
The same analyzer should be used for indexing and querying, otherwise this will result in faulty interpretation.
The SwedishAnalyzer were used at first, but it turned out to be more destructive than helpful. 
So a CustomAnalyzer, built upon the SwedishAnalyzer but without the stemming, were created.

\subsubsection{Indexing}
Searching through entire documents for relevant text is extremely inefficient, 
document indexing is a process of entering information from different documents into a searchable database. 
Lucene uses documents to differ text segments from each other, and as mentioned, both articles and paragraphs were used as documents, separately.
Eventually it was determined that indexing by paragraph was the more efficient way, due to the reduction of the irrelevant text.
Each document had two sections, title and text. Where title is the article title, and text is the text body of the subsection in question.

\subsubsection{Similarity}
An important part in the passage retriever concerning queries, is how to determine if a query is similar to a document.
Lucenes default similarity function is a variant of the tf-idf similarity, 
which is defined as the scoring of the Vector Space Model (VSM) of a document d and a query q.
\[VSM score = \frac{V(q)\cdot V(d)}{|V(q)||V(d)|} \]
Where $V(q) \cdot V(d)$ is the dot product of the weighted vectors, and $|V(q)|$ and $|V(d)|$ are their Euclidean norms.

There is another similarity function, called BM25, that scores similarity as
\[ score(q,d) = \sum_{i=1}^{|q|} idf(q_i) \cdot \frac{tf(q_i,d) \cdot (k_1 + 1)}{tf(q_1,d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})} \]
where $f(q_i,d)$ is the frequency of the term $q_i$ in document $d$, 
$|d|$ is the number of words in document $d$,
$avgdl$ is the average number of words in each document,
$k_1$ and $b$ are free parameters, default is $k_1 = 1.2$ and $b = 0.75$,
and $idf(q_i)$ is the inverse document frequency weight of the query term $q_i$.

\subsubsection{Querying}
To search the index for relevant text, both the title and the text field had to be checked. 
This was done with Lucenes MultifieldQueryParser, that checked for all the query terms in both fields: \\
\texttt{(title:term1 body:term1) (title:term2 body:term2)} \\
How similar a query was to a certain passage was determined by the BM25 similarity.
The result was a series of documents, sorted by their score.
