Out of the 2300 questions, about 1600 of them had an answer containing only one word.
When performing test on those 1600 questions, using only 100 passages per query, 
in 78\% of the cases, i.e. 1248 questions, the answer existed somewhere in the text.
Using only our simple noun ranker, the median rank for these 1248 answers were 20.

We chose to index Wikipedia in paragraphs because of less irrelevant information retrieved and better performance. Although 
indexing by whole articles had a greater chance of actually containing the answer at all.
For future research this might be reconsidered if the machine learning improves enough to push answers with very low frequency to the
top or better hardware used to yield better performance.

After improving this result using our reranker, the median rank for these 1248 questions was lowered to an astounding 2.
The puncher, still in a developing stage, had very little impact, but still managed to improve the mean value slightly.

Since this system is focused on simple questions, where the answer is a noun, there are many opportunities for improvement.
Even though 78\% is a good result for answers present, it still has room for improvement. By adapting the system more to the Swedish language, 
and fully develop a Swedish analyzer and stemmer, the percentage of answers present might increase.
Also by parsing the question in a more effective way, e.g. reducing the query down to the relevant words, could improve the querying.

The current data that is extracted from Wikipedia is simply the raw text that can bee seen in an article. 
Instead of just removing things we can't parse, the usage of another tool, like Sweble, could improve the index data itself.
Also by branching out from Wikipedia, and use other data sources, could improve the available data.

The available training set is very important for the systems performance, increasing the question set should provide a better foundation for the reranker.

Named entity classifier


